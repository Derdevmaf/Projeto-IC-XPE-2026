{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33f7118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "with open(\"data/raw/projetos_pbl_aprendizado_por_reforco.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dados = json.load(f)\n",
    "\n",
    "len(dados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4936ddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Agente no Mundo do Grelha (Gridworld)': ['MDPs',\n",
       "  'Estados',\n",
       "  'Ações',\n",
       "  'Recompensas',\n",
       "  'Políticas',\n",
       "  'Função de Valor'],\n",
       " 'Iteração de Valor e Política no FrozenLake': ['Iteração de Valor',\n",
       "  'Iteração de Política',\n",
       "  'Equação de Bellman',\n",
       "  'Convergência',\n",
       "  'OpenAI Gym (Básico)'],\n",
       " 'Q-Learning para o Caçador de Tesouros': ['Q-Learning',\n",
       "  'Exploração-Explotação (epsilon-greedy)',\n",
       "  'Tabela Q',\n",
       "  'Atualização Temporal (TD)',\n",
       "  'Taxa de Aprendizagem'],\n",
       " 'SARSA no Ambiente Taxi-v3': ['SARSA',\n",
       "  'On-policy',\n",
       "  'Off-policy',\n",
       "  'OpenAI Gym',\n",
       "  'TD(0)'],\n",
       " 'Criação de Ambiente Personalizado e Comparação de Algoritmos Tabulares': ['Design de Ambientes',\n",
       "  'Representação de Estados e Ações',\n",
       "  'Comparação de Algoritmos',\n",
       "  'Escalabilidade'],\n",
       " 'DQN para CarPole-v1': ['DQN',\n",
       "  'Redes Neurais',\n",
       "  'Replay Buffer',\n",
       "  'Rede Alvo',\n",
       "  'Experiência Replay',\n",
       "  'CarPole-v1'],\n",
       " 'Policy Gradient para Acrobot-v1': ['Policy Gradient',\n",
       "  'REINFORCE',\n",
       "  'Monte Carlo',\n",
       "  'Política Estocástica',\n",
       "  'Acrobot-v1'],\n",
       " 'Actor-Critic para BipedalWalker-v3 (versão discreta simplificada)': ['Actor-Critic',\n",
       "  'Vantagem (Advantage)',\n",
       "  'Função de Valor de Estados',\n",
       "  'Função de Política'],\n",
       " 'Estratégias de Exploração Avançadas em MountainCar': ['Exploração',\n",
       "  'Explotação',\n",
       "  'Epsilon-greedy',\n",
       "  'Ruído Gaussiano',\n",
       "  'Upper Confidence Bound (UCB)',\n",
       "  'Intrinsically Motivated Exploration'],\n",
       " 'DQN Duplo e Dueling para Ambientes Atari (Pong ou Breakout)': ['Double DQN',\n",
       "  'Dueling DQN',\n",
       "  'Jogos Atari',\n",
       "  'Representação de Imagens',\n",
       "  'Superestimação'],\n",
       " 'DDPG/TD3 para Controle Contínuo (HalfCheetah-v3)': ['DDPG',\n",
       "  'TD3',\n",
       "  'Ações Contínuas',\n",
       "  'Redes de Ator-Crítico Determinísticas',\n",
       "  'Ruído de Ação'],\n",
       " 'Aprendizado Baseado em Modelo com Dyna-Q': ['Dyna-Q',\n",
       "  'Aprendizado Baseado em Modelo',\n",
       "  'Planejamento',\n",
       "  'Simulação de Modelo',\n",
       "  'Geração de Experiências'],\n",
       " 'Aprendizado por Reforço Multiagente (MARL) em um Cenário de Cooperação/Competição': ['MARL',\n",
       "  'Jogos Multiagente',\n",
       "  'Não Estacionariedade',\n",
       "  'Crédito',\n",
       "  'Vantagem Conjunta'],\n",
       " 'Aprendizado por Reforço Hierárquico (HRL) para Tarefas Complexas': ['HRL',\n",
       "  'Opções',\n",
       "  'Metas',\n",
       "  'Decomposição de Tarefas',\n",
       "  'Aprendizado de Longo Horizonte'],\n",
       " 'Aplicação de RL em um Problema do Mundo Real (Sugestão/Pesquisa)': ['RL Aplicado',\n",
       "  'Engenharia de Prompt/Ambiente',\n",
       "  'Modelagem de Problemas',\n",
       "  'Análise de Viabilidade']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl_to_los_m1 = {}\n",
    "\n",
    "for projeto in dados:\n",
    "    nome = projeto[\"nome_do_projeto\"]\n",
    "    los = projeto[\"unidades_de_conhecimento_utilizadas\"]\n",
    "    pbl_to_los_m1[nome] = los\n",
    "\n",
    "pbl_to_los_m1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
