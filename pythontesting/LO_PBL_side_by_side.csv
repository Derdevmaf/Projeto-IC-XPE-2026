Learning Objectives,#PBLs,PBL,#Learning Objectives
"Definir os componentes fundamentais de um problema de Aprendizado por Reforço (RL), incluindo agente, ambiente, estado, ação, recompensa e política.",14,Agente no Mundo do Grelha (Gridworld),5
"Formular um problema de decisão sequencial como um Processo de Decisão de Markov (MDP), identificando seus elementos principais como estados, ações, probabilidades de transição e recompensas.",13,Iteração de Valor e Política no FrozenLake,5
Aplicar as equações de Bellman para calcular funções de valor de estado (V) e ação (Q) em MDPs dados.,6,Q-Learning para o Caçador de Tesouros,5
"Implementar algoritmos de Programação Dinâmica, como Iteração de Política e Iteração de Valor, para encontrar políticas ótimas em MDPs com modelo conhecido.",4,SARSA no Ambiente Taxi-v3,5
"Diferenciar entre métodos de Monte Carlo e Aprendizagem por Diferenças Temporais (TD(0)) para predição de valor em ambientes sem modelo, identificando suas vantagens e desvantagens.",6,Criação de Ambiente Personalizado e Comparação de Algoritmos Tabulares,5
"Implementar algoritmos de controle model-free on-policy, como SARSA, para aprender políticas ótimas em ambientes desconhecidos, considerando o dilema exploração-explotação.",5,DQN para CarPole-v1,5
"Implementar algoritmos de controle model-free off-policy, como Q-learning, para aprender políticas ótimas, compreendendo a importância da política de comportamento e da política alvo.",8,Policy Gradient para Acrobot-v1,5
Utilizar técnicas de aproximação de função linear para lidar com espaços de estados ou ações grandes e contínuos em problemas de Aprendizado por Reforço.,0,Actor-Critic para BipedalWalker-v3 (versão discreta simplificada),5
"Explicar a arquitetura e os mecanismos-chave das Redes Q Profundas (DQN), como replay de experiência e redes alvo, para estabilizar o treinamento em ambientes complexos.",3,Estratégias de Exploração Avançadas em MountainCar,5
"Implementar o algoritmo REINFORCE para otimização de política em problemas com espaços de ação discretos, utilizando a abordagem de gradientes de política.",2,DQN Duplo e Dueling para Ambientes Atari (Pong ou Breakout),5
"Desenvolver agentes Actor-Critic para ambientes com espaços de estados e/ou ações grandes, combinando os benefícios de métodos baseados em valor e em política para melhor desempenho e estabilidade.",6,DDPG/TD3 para Controle Contínuo (HalfCheetah-v3),5
"Aplicar algoritmos de Aprendizado por Reforço Profundo para problemas de controle em ambientes com espaços de ações contínuos, como DDPG ou TD3.",1,Aprendizado Baseado em Modelo com Dyna-Q,5
"Analisar e comparar diferentes estratégias de exploração avançadas, além de epsilon-greedy, como Upper Confidence Bound (UCB) e exploração baseada em curiosidade.",2,Aprendizado por Reforço Multiagente (MARL) em um Cenário de Cooperação/Competição,5
"Avaliar criticamente a adequação de diferentes algoritmos de Aprendizado por Reforço para resolver problemas do mundo real, considerando suas vantagens, limitações e requisitos computacionais.",2,Aprendizado por Reforço Hierárquico (HRL) para Tarefas Complexas,5
"Propor e justificar a seleção de um pipeline de Aprendizado por Reforço completo para um novo problema, desde a formulação até a avaliação do desempenho e o ajuste de hiperparâmetros.",3,Aplicação de RL em um Problema do Mundo Real (Sugestão/Pesquisa),5
