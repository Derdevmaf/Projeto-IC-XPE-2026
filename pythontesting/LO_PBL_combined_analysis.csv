Entity,Type,Count
Utilizar técnicas de aproximação de função linear para lidar com espaços de estados ou ações grandes e contínuos em problemas de Aprendizado por Reforço.,LO,0
"Aplicar algoritmos de Aprendizado por Reforço Profundo para problemas de controle em ambientes com espaços de ações contínuos, como DDPG ou TD3.",LO,1
"Implementar o algoritmo REINFORCE para otimização de política em problemas com espaços de ação discretos, utilizando a abordagem de gradientes de política.",LO,2
"Analisar e comparar diferentes estratégias de exploração avançadas, além de epsilon-greedy, como Upper Confidence Bound (UCB) e exploração baseada em curiosidade.",LO,2
"Avaliar criticamente a adequação de diferentes algoritmos de Aprendizado por Reforço para resolver problemas do mundo real, considerando suas vantagens, limitações e requisitos computacionais.",LO,2
"Explicar a arquitetura e os mecanismos-chave das Redes Q Profundas (DQN), como replay de experiência e redes alvo, para estabilizar o treinamento em ambientes complexos.",LO,3
"Propor e justificar a seleção de um pipeline de Aprendizado por Reforço completo para um novo problema, desde a formulação até a avaliação do desempenho e o ajuste de hiperparâmetros.",LO,3
"Implementar algoritmos de Programação Dinâmica, como Iteração de Política e Iteração de Valor, para encontrar políticas ótimas em MDPs com modelo conhecido.",LO,4
"Implementar algoritmos de controle model-free on-policy, como SARSA, para aprender políticas ótimas em ambientes desconhecidos, considerando o dilema exploração-explotação.",LO,5
Aplicar as equações de Bellman para calcular funções de valor de estado (V) e ação (Q) em MDPs dados.,LO,6
"Diferenciar entre métodos de Monte Carlo e Aprendizagem por Diferenças Temporais (TD(0)) para predição de valor em ambientes sem modelo, identificando suas vantagens e desvantagens.",LO,6
"Desenvolver agentes Actor-Critic para ambientes com espaços de estados e/ou ações grandes, combinando os benefícios de métodos baseados em valor e em política para melhor desempenho e estabilidade.",LO,6
"Implementar algoritmos de controle model-free off-policy, como Q-learning, para aprender políticas ótimas, compreendendo a importância da política de comportamento e da política alvo.",LO,8
"Formular um problema de decisão sequencial como um Processo de Decisão de Markov (MDP), identificando seus elementos principais como estados, ações, probabilidades de transição e recompensas.",LO,13
"Definir os componentes fundamentais de um problema de Aprendizado por Reforço (RL), incluindo agente, ambiente, estado, ação, recompensa e política.",LO,14
Agente no Mundo do Grelha (Gridworld),PBL,5
Iteração de Valor e Política no FrozenLake,PBL,5
Q-Learning para o Caçador de Tesouros,PBL,5
SARSA no Ambiente Taxi-v3,PBL,5
Criação de Ambiente Personalizado e Comparação de Algoritmos Tabulares,PBL,5
DQN para CarPole-v1,PBL,5
Policy Gradient para Acrobot-v1,PBL,5
Actor-Critic para BipedalWalker-v3 (versão discreta simplificada),PBL,5
Estratégias de Exploração Avançadas em MountainCar,PBL,5
DQN Duplo e Dueling para Ambientes Atari (Pong ou Breakout),PBL,5
DDPG/TD3 para Controle Contínuo (HalfCheetah-v3),PBL,5
Aprendizado Baseado em Modelo com Dyna-Q,PBL,5
Aprendizado por Reforço Multiagente (MARL) em um Cenário de Cooperação/Competição,PBL,5
Aprendizado por Reforço Hierárquico (HRL) para Tarefas Complexas,PBL,5
Aplicação de RL em um Problema do Mundo Real (Sugestão/Pesquisa),PBL,5
